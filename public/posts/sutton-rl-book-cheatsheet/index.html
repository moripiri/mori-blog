<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=64609&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Sutton의 RL 책 한 장 요약 | mori-blog</title><meta name=keywords content="RL"><meta name=description content="한 페이지로 보는 Sutton의 RL 책"><meta name=author content="mori"><link rel=canonical href=http://localhost:64609/posts/sutton-rl-book-cheatsheet/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:64609/static/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:64609/static/favicons/favicon-16x16.ico><link rel=icon type=image/png sizes=32x32 href=http://localhost:64609/static/favicons/favicon-32x32.ico><link rel=apple-touch-icon href=http://localhost:64609/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:64609/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:64609/posts/sutton-rl-book-cheatsheet/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><meta property="og:url" content="http://localhost:64609/posts/sutton-rl-book-cheatsheet/"><meta property="og:site_name" content="mori-blog"><meta property="og:title" content="Sutton의 RL 책 한 장 요약"><meta property="og:description" content="한 페이지로 보는 Sutton의 RL 책"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-15T11:30:03+00:00"><meta property="article:modified_time" content="2025-06-15T11:30:03+00:00"><meta property="article:tag" content="RL"><meta property="og:image" content="http://localhost:64609/%3Cimage%20path/url%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:64609/%3Cimage%20path/url%3E"><meta name=twitter:title content="Sutton의 RL 책 한 장 요약"><meta name=twitter:description content="한 페이지로 보는 Sutton의 RL 책"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:64609/posts/"},{"@type":"ListItem","position":2,"name":"Sutton의 RL 책 한 장 요약","item":"http://localhost:64609/posts/sutton-rl-book-cheatsheet/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Sutton의 RL 책 한 장 요약","name":"Sutton의 RL 책 한 장 요약","description":"한 페이지로 보는 Sutton의 RL 책","keywords":["RL"],"articleBody":"Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew Barto: link\nIntroduction Reinforcement Learning: An area of machine learning that aims to learn what to do to maximize cumulative reward.\nAgents and Environments At each timestep $t$:\nAgents: Receive Reward $R_t$ and Observation $O_t$, execute Action $A_t$ Environments: Receive Action $A_t$, then emit Reward $R_{t+1}$ and Next Observation $O_{t+1}$ The sequence of observations, actions, and rewards is called history.\n$$\\begin{aligned} H_t = O_1, R_1, A_1, … , A_{t-1}, O_t, R_t \\end{aligned}$$ State$(S_t)$: information used to determine what happens next. Both the agent and environment have state, and they may not agree with each other.\nFully observable environments: $O_t = S_t$. The agent can know the exact state of the environment. It is called a Markov Decision Process (MDP) Partially observable environments: $O_t \\neq S_t$. The agent only knows the partial state of the environment. It is called a Partially Observable Markov Decision Process (POMDP) Markov Decision Processes (MDP) Markov Decision Process (MDP) is an environment that can be defined as a 5-tuple:\n$$\\begin{aligned} (\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma) \\end{aligned}$$\nwhere\n$\\mathcal{S}$: state space (set of states)\n$\\mathcal{A}$: action space (set of actions)\n$\\mathcal{P}$: state transition probability matrix $\\mathcal{P}=\\mathbb{P}[S_{t+1}=s’|S_t=s, A_t=a]$\n$\\mathcal{R}$: reward function $\\mathcal{R} = \\mathbb{E}[R_{t+1}|S_t=s, A_t=a]$\n$\\gamma$: discount factor $\\gamma \\in [0, 1]$\nMarkov Property $$\\begin{aligned} \\mathbb{P}[S_{t+1}|S_t] = \\mathbb{P}[S_{t+1}|S_1, S_2, … , S_t] \\end{aligned}$$\nPolicy $$\\begin{aligned} \\pi(a|s) = \\mathbb{P}[A_{t} = a|S_t = s] \\end{aligned}$$\nReturn $$\\begin{aligned} G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} \\cdots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\end{aligned}$$\nwhere $\\gamma \\in [0, 1]$\nValue function \u0026 Action-value function (Q-value function) $$\\begin{aligned} v_\\pi(s) = \\mathbb{E_\\pi}[G_t|S_t = s] \\end{aligned}$$\n$$\\begin{aligned} q_\\pi(s, a) = \\mathbb{E_\\pi}[G_t|S_t = s, A_t = a] \\end{aligned}$$\nBellman equation Let’s derive the Bellman expectation equation for $v_\\pi(s)$.\n$$ \\begin{aligned} v_{\\pi}(s) \u0026= \\mathbb{E_\\pi}[G_t \\mid S_t = s] \\\\ \u0026= \\mathbb{E_\\pi} \\left[\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\mid S_t = s \\right] \\\\ \u0026= \\mathbb{E_\\pi} \\left[R_{t+1} + \\gamma \\sum_{k=0}^\\infty \\gamma^k R_{t+k+2} \\mid S_t = s \\right] \\\\ \u0026= \\mathbb{E_\\pi} \\left[R_{t+1} + \\gamma v_\\pi(s_{t+1}) \\mid S_t = s \\right] \\end{aligned} $$\n$$ \\begin{aligned} q_\\pi(s, a) \u0026= \\mathbb{E_\\pi}[R_{t+1} + \\gamma q_\\pi (S_{t+1}, A_{t+1}) \\mid S_t = s, A_t = a] \\end{aligned} $$\nOptimal value functions $$\\begin{aligned} \\pi \\geq \\pi’ \\text{ iff } v_\\pi (s) \\geq v_{\\pi’} (s) \\text{ for } \\forall s \\in \\mathcal{S} \\end{aligned}$$\n$$\\begin{aligned} v_*(s) = \\max_{\\pi} v_\\pi (s) \\end{aligned}$$\n$$\\begin{aligned} q_*(s, a) = \\max_{\\pi} q_\\pi (s, a) \\end{aligned}$$\nfor all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$.\n$$\\begin{aligned} v_\\ast(s) = \\max_{a} q_\\ast (s, a) \\end{aligned}$$\n$$\\begin{aligned} v_\\ast(s) = \\mathbb{E_\\pi}[R_{t+1} + \\gamma v_\\ast (S_{t+1})|S_t = s] \\end{aligned}$$\n$$\\begin{aligned} q_*(s, a) = \\mathbb{E_\\pi}[R_{t+1} + \\gamma \\max_{a’} q_\\ast (S_{t+1}, a’)|S_t = s, A_t = a] \\end{aligned}$$\nDynamic Programming What is Dynamic Programming? Dynamic programming (DP): a collection of algorithms that can be used to compute optimal policies given a perfect model such as a Markov Decision Process (MDP).\nDP can solve problems that have two properties:\nOverlapping subproblems Problem can be broken down into subproblems Solutions to subproblems can be reused Optimal substructure Problem can be decomposed into subproblems Policy Iteration Value Iteration Model-Free Prediction Model-free prediction algorithms aim to estimate the value function of a certain policy without knowing the MDP.\nMonte-Carlo Prediction Temporal-Difference Learning Difference between Monte-Carlo (MC) and Temporal-Difference (TD) Model-Free Control Model-free control algorithms can be divided into two groups: on-policy control and off-policy control.\nOn-policy control: Learn about policy $\\pi$ from experience sampled from $\\pi$\nOff-policy control: Learn about policy $\\pi$ from experience sampled from policy $\\mu \\neq \\pi$\nBoth types of control algorithms are widely utilized in RL.\nOn-policy Monte-Carlo Control On-policy Temporal-Difference Control Off-policy Temporal-Difference Control Policy Gradient Methods In policy gradient, policy parameter $\\theta$ is updated by some scalar performance measure $\\mathcal{J}(\\theta)$ with respect to the policy parameter. To maximize policy performance, their updates approximate gradient ascent in $\\mathcal{J}$:\n$$\\begin{align} \\theta_{t+1} = \\theta_t + \\alpha \\nabla \\mathcal{J}(\\theta_t) \\end{align}$$\nwhere $\\alpha$ is a step-size parameter.\nThen what can be the performance measure $\\mathcal{J}(\\theta)$ for an MDP policy in finite episodes?\nIn the episodic case, we have trajectory $\\tau$. Then, we can define $\\mathcal{J}(\\theta)$ as its value.\n$$\\begin{align} \\mathcal{J}(\\theta) = \\sum_{s \\in \\mathcal{S}} d^\\pi(s)v_\\pi(s) = \\sum_{s \\in \\mathcal{S}} d^\\pi(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(a|s) q_\\pi(s, a) \\end{align}$$\nwhere $d^\\pi$ is the stationary distribution for the Markov chain for $\\pi_\\theta$.\nThen the gradient of $\\mathcal{J}$ can be reformatted as the following:\n$$\\begin{aligned} \\nabla_\\theta \\mathcal{J}(\\theta) \u0026= \\nabla_\\theta \\sum_{s \\in \\mathcal{S}} d^\\pi(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(a|s) Q_\\pi(s, a) \u0026 \\\\ \u0026\\propto \\sum_{s \\in \\mathcal{S}} d^\\pi(s) \\sum_{a \\in \\mathcal{A}} Q_\\pi(s, a) \\nabla_\\theta \\pi_\\theta(a|s) \\\\ \u0026= \\sum_{s \\in \\mathcal{S}} d^\\pi(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(s, a) Q_\\pi(s, a) \\dfrac{\\nabla_\\theta \\pi_\\theta(a|s)}{\\pi_\\theta(a|s)} \\\\ \u0026= \\mathbb{E_\\pi} [Q_\\pi(s,a) \\nabla_\\theta \\log \\pi_\\theta(s,a)] \\end{aligned}$$\nwhere $\\mathbb{E_\\pi}$ refers to $\\mathbb{E_{s \\sim d_\\pi, a \\sim \\pi_\\theta}}$ and $Q_\\pi$ means the true state-value under policy $\\pi$.\nPolicy Based Reinforcement Learning RL algorithms that directly generate the policy from experience: Policy-based RL.\nMonte-Carlo Policy Gradient $$\\begin{align} \\mathbb{E_\\pi} [G_t|s_t, a_t] = Q_\\pi (s_t, a_t) \\end{align}$$\nActor-Critic Policy Gradient Yet the algorithm REINFORCE has a disadvantage of high gradient variance.\nThus, to reduce variance, a critic can be used instead of return to estimate $Q_\\pi$.\n$$\\begin{align} q_\\phi (s, a) = Q_\\pi(s, a) \\end{align}$$\nTherefore, the policy gradient is changed as\n$$\\begin{align} \\nabla_\\theta \\mathcal{J}(\\theta) = \\mathbb{E_\\pi} [q_\\phi(s,a) \\nabla_\\theta \\log \\pi_\\theta(s,a)]\\ \\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta (a_t|s_t) q_\\phi(s_t, a_t) \\end{align}$$\nCritic can be updated by\nMonte-Carlo evaluation TD(0) TD($\\lambda$) For example, if we use TD(0), then in timestep $t$ the critic will be updated as\n$$\\begin{align} \\phi \\leftarrow \\phi + \\beta (r_t + \\gamma q_\\phi(s_{t+1}, a_{t+1}) - q_\\phi(s_t, a_t)) \\nabla_\\phi q_\\phi(s_t, a_t) \\end{align}$$\nModel-based Reinforcement Learning Model-based RL: it learns a model from experience and plans value function and/or policy from the model.\nWhat is a Model? A Model $\\mathcal{M}$ is a representation of an MDP \u003c$\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}$\u003e parametrized by $\\eta$. Assuming we know the state space $\\mathcal{S}$ and action space $\\mathcal{A}$, a model $\u003c\\mathcal{P}, \\mathcal{R}\u003e$ can represent transitions\n$$\\begin{align} S_{t+1} \\sim \\mathcal{P_\\eta(S_{t+1}|S_t, A_t)} \\\\ R_{t+1} \\sim \\mathcal{R_\\eta(R_{t+1}|S_t, A_t)} \\end{align}$$\nThe model $\u003c\\mathcal{P}, \\mathcal{R}\u003e$ is learned from experience ${S_1, A_1, R_2, … , S_T}$.\nA model can be parametrized in various ways: from lookup table models to deep neural networks.\n","wordCount":"1028","inLanguage":"en","image":"http://localhost:64609/%3Cimage%20path/url%3E","datePublished":"2025-06-15T11:30:03Z","dateModified":"2025-06-15T11:30:03Z","author":{"@type":"Person","name":"mori"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:64609/posts/sutton-rl-book-cheatsheet/"},"publisher":{"@type":"Organization","name":"mori-blog","logo":{"@type":"ImageObject","url":"http://localhost:64609/static/favicons/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:64609/ accesskey=h title="Mori's blog (Alt + H)"><img src=http://localhost:64609/apple-touch-icon.png alt aria-label=logo height=35>Mori's blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:64609/posts/ title=posts><span>posts</span></a></li><li><a href=http://localhost:64609/tags/ title=tags><span>tags</span></a></li><li><a href=http://localhost:64609/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css integrity=sha384-t5CR+zwDAROtph0PXGte6ia8heboACF9R5l/DiY+WZ3P2lxNgvJkQk5n7GPvLMYw crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js integrity=sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8+w2LAIftJEULZABrF9PPFv+tVkH crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js integrity=sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB+w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script type=module>
    import renderMathInElement from "https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.mjs";
    renderMathInElement(document.body);
</script><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:64609/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:64609/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Sutton의 RL 책 한 장 요약</h1><div class=post-description>한 페이지로 보는 Sutton의 RL 책</div><div class=post-meta><span title='2025-06-15 11:30:03 +0000 +0000'>June 15, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;1028 words&nbsp;·&nbsp;mori&nbsp;|&nbsp;<a href=https://github.com/moripiri/moripiri.github.io/issues/posts/Sutton-RL-book-cheatsheet/index.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#agents-and-environments>Agents and Environments</a></li><li><a href=#markov-decision-processes-mdp>Markov Decision Processes (MDP)</a></li></ul></li><li><a href=#dynamic-programming>Dynamic Programming</a><ul><li><a href=#what-is-dynamic-programming>What is Dynamic Programming?</a></li><li><a href=#policy-iteration>Policy Iteration</a></li><li><a href=#value-iteration>Value Iteration</a></li></ul></li><li><a href=#model-free-prediction>Model-Free Prediction</a><ul><li><a href=#monte-carlo-prediction>Monte-Carlo Prediction</a></li><li><a href=#temporal-difference-learning>Temporal-Difference Learning</a></li><li><a href=#difference-between-monte-carlo-mc-and-temporal-difference-td>Difference between Monte-Carlo (MC) and Temporal-Difference (TD)</a></li></ul></li><li><a href=#model-free-control>Model-Free Control</a><ul><li><a href=#on-policy-monte-carlo-control>On-policy Monte-Carlo Control</a></li><li><a href=#on-policy-temporal-difference-control>On-policy Temporal-Difference Control</a></li><li><a href=#off-policy-temporal-difference-control>Off-policy Temporal-Difference Control</a></li></ul></li><li><a href=#policy-gradient-methods>Policy Gradient Methods</a><ul><li><a href=#policy-based-reinforcement-learning>Policy Based Reinforcement Learning</a></li><li><a href=#monte-carlo-policy-gradient>Monte-Carlo Policy Gradient</a></li><li><a href=#actor-critic-policy-gradient>Actor-Critic Policy Gradient</a></li></ul></li><li><a href=#model-based-reinforcement-learning>Model-based Reinforcement Learning</a><ul><li><a href=#what-is-a-model>What is a Model?</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p><strong>Reinforcement Learning: An Introduction</strong> by Richard S. Sutton and Andrew Barto: <a href=https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf>link</a></p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Reinforcement Learning: An area of machine learning that aims to learn what to do to maximize cumulative reward.</p><h3 id=agents-and-environments>Agents and Environments<a hidden class=anchor aria-hidden=true href=#agents-and-environments>#</a></h3><p align=center><img src=images/agent_environment.png></p><p>At each timestep $t$:</p><ul><li><strong>Agents</strong>: Receive Reward $R_t$ and Observation $O_t$, execute Action $A_t$</li><li><strong>Environments</strong>: Receive Action $A_t$, then emit Reward $R_{t+1}$ and Next Observation $O_{t+1}$</li></ul><p>The sequence of observations, actions, and rewards is called history.</br></p><p>$$\begin{aligned}
H_t = O_1, R_1, A_1, &mldr; , A_{t-1}, O_t, R_t
\end{aligned}$$</br></br><strong>State</strong>$(S_t)$: information used to determine what happens next. Both the agent and environment have state, and they may not agree with each other.</p><ul><li><strong>Fully observable environments</strong>: $O_t = S_t$. The agent can know the exact state of the environment. It is called a <strong>Markov Decision Process (MDP)</strong></li><li><strong>Partially observable environments</strong>: $O_t \neq S_t$. The agent only knows the partial state of the environment. It is called a <strong>Partially Observable Markov Decision Process (POMDP)</strong></li></ul><h3 id=markov-decision-processes-mdp>Markov Decision Processes (MDP)<a hidden class=anchor aria-hidden=true href=#markov-decision-processes-mdp>#</a></h3><p><strong>Markov Decision Process (MDP)</strong> is an environment that can be defined as a 5-tuple:</p><p>$$\begin{aligned}
(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)
\end{aligned}$$</p><p>where</p><ul><li><p>$\mathcal{S}$: state space (set of states)</p></li><li><p>$\mathcal{A}$: action space (set of actions)</p></li><li><p>$\mathcal{P}$: state transition probability matrix $\mathcal{P}=\mathbb{P}[S_{t+1}=s&rsquo;|S_t=s, A_t=a]$</p></li><li><p>$\mathcal{R}$: reward function $\mathcal{R} = \mathbb{E}[R_{t+1}|S_t=s, A_t=a]$</p></li><li><p>$\gamma$: discount factor $\gamma \in [0, 1]$</p></li></ul><h4 id=markov-property>Markov Property<a hidden class=anchor aria-hidden=true href=#markov-property>#</a></h4><p>$$\begin{aligned}
\mathbb{P}[S_{t+1}|S_t] = \mathbb{P}[S_{t+1}|S_1, S_2, &mldr; , S_t]
\end{aligned}$$</p><h4 id=policy>Policy<a hidden class=anchor aria-hidden=true href=#policy>#</a></h4><p>$$\begin{aligned}
\pi(a|s) = \mathbb{P}[A_{t} = a|S_t = s]
\end{aligned}$$</p><h4 id=return>Return<a hidden class=anchor aria-hidden=true href=#return>#</a></h4><p>$$\begin{aligned}
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} \cdots = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
\end{aligned}$$</p><p>where $\gamma \in [0, 1]$</p><h4 id=value-function--action-value-function-q-value-function>Value function & Action-value function (Q-value function)<a hidden class=anchor aria-hidden=true href=#value-function--action-value-function-q-value-function>#</a></h4><p>$$\begin{aligned}
v_\pi(s) = \mathbb{E_\pi}[G_t|S_t = s]
\end{aligned}$$</p><p>$$\begin{aligned}
q_\pi(s, a) = \mathbb{E_\pi}[G_t|S_t = s, A_t = a]
\end{aligned}$$</p><h4 id=bellman-equation>Bellman equation<a hidden class=anchor aria-hidden=true href=#bellman-equation>#</a></h4><p>Let&rsquo;s derive the Bellman expectation equation for $v_\pi(s)$.</p><p>$$
\begin{aligned}
v_{\pi}(s) &= \mathbb{E_\pi}[G_t \mid S_t = s] \\
&= \mathbb{E_\pi} \left[\sum_{k=0}^\infty \gamma^k R_{t+k+1} \mid S_t = s \right] \\
&= \mathbb{E_\pi} \left[R_{t+1} + \gamma \sum_{k=0}^\infty \gamma^k R_{t+k+2} \mid S_t = s \right] \\
&= \mathbb{E_\pi} \left[R_{t+1} + \gamma v_\pi(s_{t+1}) \mid S_t = s \right]
\end{aligned}
$$</p><p>$$
\begin{aligned}
q_\pi(s, a) &= \mathbb{E_\pi}[R_{t+1} + \gamma q_\pi (S_{t+1}, A_{t+1}) \mid S_t = s, A_t = a]
\end{aligned}
$$</p><h4 id=optimal-value-functions>Optimal value functions<a hidden class=anchor aria-hidden=true href=#optimal-value-functions>#</a></h4><p>$$\begin{aligned}
\pi \geq \pi&rsquo; \text{ iff } v_\pi (s) \geq v_{\pi&rsquo;} (s) \text{ for } \forall s \in \mathcal{S}
\end{aligned}$$</p><p>$$\begin{aligned}
v_*(s) = \max_{\pi} v_\pi (s)
\end{aligned}$$</p><p>$$\begin{aligned}
q_*(s, a) = \max_{\pi} q_\pi (s, a)
\end{aligned}$$</p><p>for all $s \in \mathcal{S}$ and $a \in \mathcal{A}$.</p><p>$$\begin{aligned}
v_\ast(s) = \max_{a} q_\ast (s, a)
\end{aligned}$$</p><p>$$\begin{aligned}
v_\ast(s) = \mathbb{E_\pi}[R_{t+1} + \gamma v_\ast (S_{t+1})|S_t = s]
\end{aligned}$$</p><p>$$\begin{aligned}
q_*(s, a) = \mathbb{E_\pi}[R_{t+1} + \gamma \max_{a&rsquo;} q_\ast (S_{t+1}, a&rsquo;)|S_t = s, A_t = a]
\end{aligned}$$</p><h2 id=dynamic-programming>Dynamic Programming<a hidden class=anchor aria-hidden=true href=#dynamic-programming>#</a></h2><h3 id=what-is-dynamic-programming>What is Dynamic Programming?<a hidden class=anchor aria-hidden=true href=#what-is-dynamic-programming>#</a></h3><p><strong>Dynamic programming (DP)</strong>: a collection of algorithms that can be used to compute optimal <strong>policies</strong> given a perfect model such as a <strong>Markov Decision Process (MDP).</strong></p><p>DP can solve problems that have two properties:</p><ol><li><strong>Overlapping subproblems</strong><ul><li>Problem can be broken down into subproblems</li><li>Solutions to subproblems can be reused</li></ul></li><li><strong>Optimal substructure</strong><ul><li>Problem can be decomposed into subproblems</li></ul></li></ol><h3 id=policy-iteration>Policy Iteration<a hidden class=anchor aria-hidden=true href=#policy-iteration>#</a></h3><p align=center><img src=images/policy_iteration_pseudocode.png></p><h3 id=value-iteration>Value Iteration<a hidden class=anchor aria-hidden=true href=#value-iteration>#</a></h3><p align=center><img src=images/value_iteration_psuedocode.png></p><h2 id=model-free-prediction>Model-Free Prediction<a hidden class=anchor aria-hidden=true href=#model-free-prediction>#</a></h2><p><strong>Model-free prediction</strong> algorithms aim to estimate the <strong>value function of a certain policy</strong> <em>without knowing the MDP</em>.</p><h3 id=monte-carlo-prediction>Monte-Carlo Prediction<a hidden class=anchor aria-hidden=true href=#monte-carlo-prediction>#</a></h3><p align=center><img src=images/mc_prediction_pseudocode.png></p><h3 id=temporal-difference-learning>Temporal-Difference Learning<a hidden class=anchor aria-hidden=true href=#temporal-difference-learning>#</a></h3><p align=center><img src=images/td_learning_pseudocode.png></p><h3 id=difference-between-monte-carlo-mc-and-temporal-difference-td>Difference between Monte-Carlo (MC) and Temporal-Difference (TD)<a hidden class=anchor aria-hidden=true href=#difference-between-monte-carlo-mc-and-temporal-difference-td>#</a></h3><p align=center><img src=images/dp_and_mc.png></p><h2 id=model-free-control>Model-Free Control<a hidden class=anchor aria-hidden=true href=#model-free-control>#</a></h2><p>Model-free control algorithms can be divided into two groups: on-policy control and off-policy control.</p><ul><li><p><strong>On-policy control</strong>: Learn about policy $\pi$ from experience sampled from $\pi$</p></li><li><p><strong>Off-policy control</strong>: Learn about policy $\pi$ from experience sampled from policy $\mu \neq \pi$</p></li></ul><p>Both types of control algorithms are widely utilized in RL.</p><h3 id=on-policy-monte-carlo-control>On-policy Monte-Carlo Control<a hidden class=anchor aria-hidden=true href=#on-policy-monte-carlo-control>#</a></h3><p align=center><img src=images/on_policy_mc_pseudocode.png></p><h3 id=on-policy-temporal-difference-control>On-policy Temporal-Difference Control<a hidden class=anchor aria-hidden=true href=#on-policy-temporal-difference-control>#</a></h3><p align=center><img src=images/sarsa_psuedocode.png></p><h3 id=off-policy-temporal-difference-control>Off-policy Temporal-Difference Control<a hidden class=anchor aria-hidden=true href=#off-policy-temporal-difference-control>#</a></h3><p align=center><img src=images/q_learning_psuedocode.png></p><h2 id=policy-gradient-methods>Policy Gradient Methods<a hidden class=anchor aria-hidden=true href=#policy-gradient-methods>#</a></h2><p>In policy gradient, policy parameter $\theta$ is updated by some scalar performance measure $\mathcal{J}(\theta)$ with respect to the policy parameter. To maximize policy performance, their updates approximate <strong>gradient ascent</strong> in $\mathcal{J}$:</p><p>$$\begin{align}
\theta_{t+1} = \theta_t + \alpha \nabla \mathcal{J}(\theta_t)
\end{align}$$</p><p>where $\alpha$ is a step-size parameter.</p><p>Then what can be the performance measure $\mathcal{J}(\theta)$ for an MDP policy in finite episodes?</p><p>In the episodic case, we have trajectory $\tau$. Then, we can define $\mathcal{J}(\theta)$ as its value.</p><p>$$\begin{align}
\mathcal{J}(\theta) = \sum_{s \in \mathcal{S}} d^\pi(s)v_\pi(s) = \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a|s) q_\pi(s, a)
\end{align}$$</p><p>where $d^\pi$ is the stationary distribution for the Markov chain for $\pi_\theta$.</p><p>Then the gradient of $\mathcal{J}$ can be reformatted as the following:</p><p>$$\begin{aligned}
\nabla_\theta \mathcal{J}(\theta) &= \nabla_\theta \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a|s) Q_\pi(s, a) & \\
&\propto \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q_\pi(s, a) \nabla_\theta \pi_\theta(a|s) \\
&= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(s, a) Q_\pi(s, a) \dfrac{\nabla_\theta \pi_\theta(a|s)}{\pi_\theta(a|s)} \\
&= \mathbb{E_\pi} [Q_\pi(s,a) \nabla_\theta \log \pi_\theta(s,a)]
\end{aligned}$$</p><p>where $\mathbb{E_\pi}$ refers to $\mathbb{E_{s \sim d_\pi, a \sim \pi_\theta}}$ and $Q_\pi$ means the <strong>true state-value</strong> under policy $\pi$.</p><h3 id=policy-based-reinforcement-learning>Policy Based Reinforcement Learning<a hidden class=anchor aria-hidden=true href=#policy-based-reinforcement-learning>#</a></h3><p>RL algorithms that directly generate the policy from experience: <strong>Policy-based RL</strong>.</p><h3 id=monte-carlo-policy-gradient>Monte-Carlo Policy Gradient<a hidden class=anchor aria-hidden=true href=#monte-carlo-policy-gradient>#</a></h3><p>$$\begin{align}
\mathbb{E_\pi} [G_t|s_t, a_t] = Q_\pi (s_t, a_t)
\end{align}$$</p><p align=center><img src=images/reinforce_pseudocode.png></p><h3 id=actor-critic-policy-gradient>Actor-Critic Policy Gradient<a hidden class=anchor aria-hidden=true href=#actor-critic-policy-gradient>#</a></h3><p>Yet the algorithm <strong>REINFORCE</strong> has a disadvantage of high gradient variance.</p><p>Thus, to reduce variance, a <strong>critic</strong> can be used instead of <strong>return</strong> to estimate $Q_\pi$.</p><p>$$\begin{align}
q_\phi (s, a) = Q_\pi(s, a)
\end{align}$$</p><p>Therefore, the policy gradient is changed as</p><p>$$\begin{align}
\nabla_\theta \mathcal{J}(\theta) = \mathbb{E_\pi} [q_\phi(s,a) \nabla_\theta \log \pi_\theta(s,a)]\
\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta (a_t|s_t) q_\phi(s_t, a_t)
\end{align}$$</p><p>Critic can be updated by</p><ul><li>Monte-Carlo evaluation</li><li>TD(0)</li><li>TD($\lambda$)</li></ul><p>For example, if we use TD(0), then in timestep $t$ the critic will be updated as</p><p>$$\begin{align}
\phi \leftarrow \phi + \beta (r_t + \gamma q_\phi(s_{t+1}, a_{t+1}) - q_\phi(s_t, a_t)) \nabla_\phi q_\phi(s_t, a_t)
\end{align}$$</p><p align=center><img src=images/actor_critic_pseudocode.png></p><h2 id=model-based-reinforcement-learning>Model-based Reinforcement Learning<a hidden class=anchor aria-hidden=true href=#model-based-reinforcement-learning>#</a></h2><p><strong>Model-based RL</strong>: it learns a model from experience and plans value function and/or policy from the model.</p><h3 id=what-is-a-model>What is a Model?<a hidden class=anchor aria-hidden=true href=#what-is-a-model>#</a></h3><p>A <em>Model</em> $\mathcal{M}$ is a representation of an MDP &lt;$\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}$> parametrized by $\eta$. Assuming we know the state space $\mathcal{S}$ and action space $\mathcal{A}$, a model $&lt;\mathcal{P}, \mathcal{R}>$ can represent transitions</p><p>$$\begin{align}
S_{t+1} \sim \mathcal{P_\eta(S_{t+1}|S_t, A_t)} \\
R_{t+1} \sim \mathcal{R_\eta(R_{t+1}|S_t, A_t)}
\end{align}$$</p><p>The model $&lt;\mathcal{P}, \mathcal{R}>$ is learned from experience ${S_1, A_1, R_2, &mldr; , S_T}$.</p><p>A model can be parametrized in various ways: from lookup table models to deep neural networks.</p><p align=center><img src=images/dyna_pseudocode.png></p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:64609/tags/rl/>RL</a></li></ul><nav class=paginav><a class=next href=http://localhost:64609/posts/my-first-post/><span class=title>Next »</span><br><span>이거 어떻게 만든 거에요?</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:64609/>mori-blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>